{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e7d5d1-bdd7-4d39-8982-de47c2df69aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 10), ('Bob', 20), ('Alice', 10), ('Charlie', 30)]\n+-------+-----+\n|   Name|Score|\n+-------+-----+\n|  Alice|   10|\n|    Bob|   20|\n|  Alice|   10|\n|Charlie|   30|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AllTransformations\").getOrCreate()\n",
    "\n",
    "# Sample RDD and DataFrame\n",
    "rdd = spark.sparkContext.parallelize([(\"Alice\", 10), (\"Bob\", 20), (\"Alice\", 10), (\"Charlie\", 30)])\n",
    "df = spark.createDataFrame([(\"Alice\", 10), (\"Bob\", 20), (\"Alice\", 10), (\"Charlie\", 30)], [\"Name\", \"Score\"])\n",
    "\n",
    "print(rdd.collect())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d094752-3d97-48d8-abf2-a440b83d12b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 20), ('Bob', 40), ('Alice', 20), ('Charlie', 60)]\n"
     ]
    }
   ],
   "source": [
    "# map() - Narrow\n",
    "\n",
    "# Applies the lambda function to each element of the RDD.\n",
    "# map() is an RDD transformation\n",
    "\n",
    "mapped_rdd = rdd.map(lambda x: (x[0], x[1] * 2))\n",
    "print(mapped_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c262207-dd2d-4686-81d9-bf2bdcded66c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bob', 20), ('Charlie', 30)]\n"
     ]
    }
   ],
   "source": [
    "# filter() - Narrow\n",
    "# Keeps only the elements of the RDD that satisfy the condition.\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] > 15)\n",
    "print(filtered_rdd.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d40b32-7571-49f5-bd39-5446a3c1c3fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'good', 'morning']\n"
     ]
    }
   ],
   "source": [
    "# flatMap() - Narrow\n",
    "\n",
    "# Similar to map(), but flattens the results.\n",
    "rdd_words = spark.sparkContext.parallelize([\"hello world\", \"good morning\"])\n",
    "\n",
    "flat_mapped_rdd = rdd_words.flatMap(lambda x: x.split(\" \"))\n",
    "print(flat_mapped_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f44819a-594e-4fa3-9912-5207b460fba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------+\n|   Name|Score|DoubleScore|\n+-------+-----+-----------+\n|  Alice|   10|         20|\n|    Bob|   20|         40|\n|  Alice|   10|         20|\n|Charlie|   30|         60|\n+-------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# withColumn() - Narrow\n",
    "\n",
    "# creates a new column DoubleScore that is Score * 2\n",
    "df_with_new = df.withColumn(\"DoubleScore\", col(\"Score\") * 2)\n",
    "df_with_new.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b92f5358-5a4d-43d9-b74e-c3d5df996b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|   Name|\n+-------+\n|  Alice|\n|    Bob|\n|  Alice|\n|Charlie|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# select() - Narrow\n",
    "\n",
    "# Retrieve specific columns from a DataFrame.\n",
    "df_selected = df.select(\"Name\")\n",
    "df_selected.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ddad8b-a4e1-46a8-82b6-b296c9cd6dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# coalesce() - Narrow\n",
    "\n",
    "# Avoids full data shuffle by just merging existing partitions.\n",
    "df_coalesced = df.coalesce(1)\n",
    "print(df_coalesced.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4367242-cac0-4e0c-a161-fa571ae08407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 10), ('Bob', 20), ('Alice', 10), ('Charlie', 30), ('David', 40)]\n+-------+-----+\n|   Name|Score|\n+-------+-----+\n|  Alice|   10|\n|    Bob|   20|\n|  Alice|   10|\n|Charlie|   30|\n|Charlie|   35|\n|  David|   40|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# union() - Wide\n",
    "\n",
    "# may involve a shuffle, depending on partition alignment.\n",
    "rdd2 = spark.sparkContext.parallelize([(\"David\", 40)])\n",
    "union_rdd = rdd.union(rdd2)\n",
    "print(union_rdd.collect())\n",
    "\n",
    "\n",
    "df2 = spark.createDataFrame([(\"Charlie\", 35), (\"David\", 40)], [\"Name\", \"Age\"])\n",
    "\n",
    "\n",
    "df_union = df.union(df2)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e116acf-dfae-498f-ad5b-14cd3d9ef3f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bob', 20), ('Charlie', 30), ('Alice', 10)]\n+-------+-----+\n|   Name|Score|\n+-------+-----+\n|  Alice|   10|\n|    Bob|   20|\n|Charlie|   30|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# distinct() - Wide\n",
    "\n",
    "# Removes duplicate rows or elements.\n",
    "distinct_rdd = rdd.distinct()\n",
    "print(distinct_rdd.collect())\n",
    "\n",
    "distinct_df = df.distinct()\n",
    "distinct_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "narrow wide",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}