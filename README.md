# Pyspark

Spark Introduction

1) Distributed System
2) Hadoop, MapReduce and Spark
3) Spark - RDD, DataFrame 
4) Pyspark

Steps to run PySpark and Jupyter Notebook using Docker

1) Install Docker desktop app
2) Create a new folder on your system
3) Place the file docker-compose.yaml
4) Run this file using command docker-compose up 
5) Open the URL http://127.0.0.1:8888/?token=YOUR_TOKEN


